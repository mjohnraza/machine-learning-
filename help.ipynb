{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d08ffd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda\n",
    "import pandas as p\n",
    "import matplotlib.pyplot as g\n",
    "import seaborn as s\n",
    "import numpy as n \n",
    "from collections import Counter as c\n",
    "\n",
    "# pipeline \n",
    "from sklearn.pipeline import Pipeline as pipe\n",
    "\n",
    "# imputation \n",
    "from sklearn.impute import SimpleImputer as si\n",
    "\n",
    "# scaling \n",
    "from sklearn.preprocessing import StandardScaler as ss , LabelEncoder as le , OrdinalEncoder as oe , OneHotEncoder as ohe \n",
    "\n",
    "\n",
    "\n",
    "# transformation\n",
    "from sklearn.compose import ColumnTransformer as ct\n",
    "\n",
    "# model selection \n",
    "from sklearn.model_selection import train_test_split as tt , cross_val_score as cv , LeaveOneOut as loo , KFold as kf , RandomizedSearchCV as rc\n",
    "\n",
    "\n",
    "\n",
    "# models \n",
    "from xgboost import XGBClassifier as xg  \n",
    "from sklearn.ensemble import AdaBoostClassifier as ad , RandomForestClassifier as rf , VotingClassifier as vc\n",
    "from sklearn.tree import DecisionTreeClassifier as dt\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn \n",
    "\n",
    " \n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score as acc   , confusion_matrix as cm  , classification_report as cr \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9979719e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized search on hyper parameters.\n",
      "\n",
      "    RandomizedSearchCV implements a \"fit\" and a \"score\" method.\n",
      "    It also implements \"score_samples\", \"predict\", \"predict_proba\",\n",
      "    \"decision_function\", \"transform\" and \"inverse_transform\" if they are\n",
      "    implemented in the estimator used.\n",
      "\n",
      "    The parameters of the estimator used to apply these methods are optimized\n",
      "    by cross-validated search over parameter settings.\n",
      "\n",
      "    In contrast to GridSearchCV, not all parameter values are tried out, but\n",
      "    rather a fixed number of parameter settings is sampled from the specified\n",
      "    distributions. The number of parameter settings that are tried is\n",
      "    given by n_iter.\n",
      "\n",
      "    If all parameters are presented as a list,\n",
      "    sampling without replacement is performed. If at least one parameter\n",
      "    is given as a distribution, sampling with replacement is used.\n",
      "    It is highly recommended to use continuous distributions for continuous\n",
      "    parameters.\n",
      "\n",
      "    Read more in the :ref:`User Guide <randomized_parameter_search>`.\n",
      "\n",
      "    .. versionadded:: 0.14\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    estimator : estimator object\n",
      "        An object of that type is instantiated for each grid point.\n",
      "        This is assumed to implement the scikit-learn estimator interface.\n",
      "        Either estimator needs to provide a ``score`` function,\n",
      "        or ``scoring`` must be passed.\n",
      "\n",
      "    param_distributions : dict or list of dicts\n",
      "        Dictionary with parameters names (`str`) as keys and distributions\n",
      "        or lists of parameters to try. Distributions must provide a ``rvs``\n",
      "        method for sampling (such as those from scipy.stats.distributions).\n",
      "        If a list is given, it is sampled uniformly.\n",
      "        If a list of dicts is given, first a dict is sampled uniformly, and\n",
      "        then a parameter is sampled using that dict as above.\n",
      "\n",
      "    n_iter : int, default=10\n",
      "        Number of parameter settings that are sampled. n_iter trades\n",
      "        off runtime vs quality of the solution.\n",
      "\n",
      "    scoring : str, callable, list, tuple or dict, default=None\n",
      "        Strategy to evaluate the performance of the cross-validated model on\n",
      "        the test set.\n",
      "\n",
      "        If `scoring` represents a single score, one can use:\n",
      "\n",
      "        - a single string (see :ref:`scoring_parameter`);\n",
      "        - a callable (see :ref:`scoring`) that returns a single value.\n",
      "\n",
      "        If `scoring` represents multiple scores, one can use:\n",
      "\n",
      "        - a list or tuple of unique strings;\n",
      "        - a callable returning a dictionary where the keys are the metric\n",
      "          names and the values are the metric scores;\n",
      "        - a dictionary with metric names as keys and callables a values.\n",
      "\n",
      "        See :ref:`multimetric_grid_search` for an example.\n",
      "\n",
      "        If None, the estimator's score method is used.\n",
      "\n",
      "    n_jobs : int, default=None\n",
      "        Number of jobs to run in parallel.\n",
      "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "        for more details.\n",
      "\n",
      "        .. versionchanged:: v0.20\n",
      "           `n_jobs` default changed from 1 to None\n",
      "\n",
      "    refit : bool, str, or callable, default=True\n",
      "        Refit an estimator using the best found parameters on the whole\n",
      "        dataset.\n",
      "\n",
      "        For multiple metric evaluation, this needs to be a `str` denoting the\n",
      "        scorer that would be used to find the best parameters for refitting\n",
      "        the estimator at the end.\n",
      "\n",
      "        Where there are considerations other than maximum score in\n",
      "        choosing a best estimator, ``refit`` can be set to a function which\n",
      "        returns the selected ``best_index_`` given the ``cv_results``. In that\n",
      "        case, the ``best_estimator_`` and ``best_params_`` will be set\n",
      "        according to the returned ``best_index_`` while the ``best_score_``\n",
      "        attribute will not be available.\n",
      "\n",
      "        The refitted estimator is made available at the ``best_estimator_``\n",
      "        attribute and permits using ``predict`` directly on this\n",
      "        ``RandomizedSearchCV`` instance.\n",
      "\n",
      "        Also for multiple metric evaluation, the attributes ``best_index_``,\n",
      "        ``best_score_`` and ``best_params_`` will only be available if\n",
      "        ``refit`` is set and all of them will be determined w.r.t this specific\n",
      "        scorer.\n",
      "\n",
      "        See ``scoring`` parameter to know more about multiple metric\n",
      "        evaluation.\n",
      "\n",
      "        .. versionchanged:: 0.20\n",
      "            Support for callable added.\n",
      "\n",
      "    cv : int, cross-validation generator or an iterable, default=None\n",
      "        Determines the cross-validation splitting strategy.\n",
      "        Possible inputs for cv are:\n",
      "\n",
      "        - None, to use the default 5-fold cross validation,\n",
      "        - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
      "        - :term:`CV splitter`,\n",
      "        - An iterable yielding (train, test) splits as arrays of indices.\n",
      "\n",
      "        For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
      "        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "        other cases, :class:`KFold` is used. These splitters are instantiated\n",
      "        with `shuffle=False` so the splits will be the same across calls.\n",
      "\n",
      "        Refer :ref:`User Guide <cross_validation>` for the various\n",
      "        cross-validation strategies that can be used here.\n",
      "\n",
      "        .. versionchanged:: 0.22\n",
      "            ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "\n",
      "    verbose : int\n",
      "        Controls the verbosity: the higher, the more messages.\n",
      "\n",
      "        - >1 : the computation time for each fold and parameter candidate is\n",
      "          displayed;\n",
      "        - >2 : the score is also displayed;\n",
      "        - >3 : the fold and candidate parameter indexes are also displayed\n",
      "          together with the starting time of the computation.\n",
      "\n",
      "    pre_dispatch : int, or str, default='2*n_jobs'\n",
      "        Controls the number of jobs that get dispatched during parallel\n",
      "        execution. Reducing this number can be useful to avoid an\n",
      "        explosion of memory consumption when more jobs get dispatched\n",
      "        than CPUs can process. This parameter can be:\n",
      "\n",
      "            - None, in which case all the jobs are immediately\n",
      "              created and spawned. Use this for lightweight and\n",
      "              fast-running jobs, to avoid delays due to on-demand\n",
      "              spawning of the jobs\n",
      "\n",
      "            - An int, giving the exact number of total jobs that are\n",
      "              spawned\n",
      "\n",
      "            - A str, giving an expression as a function of n_jobs,\n",
      "              as in '2*n_jobs'\n",
      "\n",
      "    random_state : int, RandomState instance or None, default=None\n",
      "        Pseudo random number generator state used for random uniform sampling\n",
      "        from lists of possible values instead of scipy.stats distributions.\n",
      "        Pass an int for reproducible output across multiple\n",
      "        function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "\n",
      "    error_score : 'raise' or numeric, default=np.nan\n",
      "        Value to assign to the score if an error occurs in estimator fitting.\n",
      "        If set to 'raise', the error is raised. If a numeric value is given,\n",
      "        FitFailedWarning is raised. This parameter does not affect the refit\n",
      "        step, which will always raise the error.\n",
      "\n",
      "    return_train_score : bool, default=False\n",
      "        If ``False``, the ``cv_results_`` attribute will not include training\n",
      "        scores.\n",
      "        Computing training scores is used to get insights on how different\n",
      "        parameter settings impact the overfitting/underfitting trade-off.\n",
      "        However computing the scores on the training set can be computationally\n",
      "        expensive and is not strictly required to select the parameters that\n",
      "        yield the best generalization performance.\n",
      "\n",
      "        .. versionadded:: 0.19\n",
      "\n",
      "        .. versionchanged:: 0.21\n",
      "            Default value was changed from ``True`` to ``False``\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    cv_results_ : dict of numpy (masked) ndarrays\n",
      "        A dict with keys as column headers and values as columns, that can be\n",
      "        imported into a pandas ``DataFrame``.\n",
      "\n",
      "        For instance the below given table\n",
      "\n",
      "        +--------------+-------------+-------------------+---+---------------+\n",
      "        | param_kernel | param_gamma | split0_test_score |...|rank_test_score|\n",
      "        +==============+=============+===================+===+===============+\n",
      "        |    'rbf'     |     0.1     |       0.80        |...|       1       |\n",
      "        +--------------+-------------+-------------------+---+---------------+\n",
      "        |    'rbf'     |     0.2     |       0.84        |...|       3       |\n",
      "        +--------------+-------------+-------------------+---+---------------+\n",
      "        |    'rbf'     |     0.3     |       0.70        |...|       2       |\n",
      "        +--------------+-------------+-------------------+---+---------------+\n",
      "\n",
      "        will be represented by a ``cv_results_`` dict of::\n",
      "\n",
      "            {\n",
      "            'param_kernel' : masked_array(data = ['rbf', 'rbf', 'rbf'],\n",
      "                                          mask = False),\n",
      "            'param_gamma'  : masked_array(data = [0.1 0.2 0.3], mask = False),\n",
      "            'split0_test_score'  : [0.80, 0.84, 0.70],\n",
      "            'split1_test_score'  : [0.82, 0.50, 0.70],\n",
      "            'mean_test_score'    : [0.81, 0.67, 0.70],\n",
      "            'std_test_score'     : [0.01, 0.24, 0.00],\n",
      "            'rank_test_score'    : [1, 3, 2],\n",
      "            'split0_train_score' : [0.80, 0.92, 0.70],\n",
      "            'split1_train_score' : [0.82, 0.55, 0.70],\n",
      "            'mean_train_score'   : [0.81, 0.74, 0.70],\n",
      "            'std_train_score'    : [0.01, 0.19, 0.00],\n",
      "            'mean_fit_time'      : [0.73, 0.63, 0.43],\n",
      "            'std_fit_time'       : [0.01, 0.02, 0.01],\n",
      "            'mean_score_time'    : [0.01, 0.06, 0.04],\n",
      "            'std_score_time'     : [0.00, 0.00, 0.00],\n",
      "            'params'             : [{'kernel' : 'rbf', 'gamma' : 0.1}, ...],\n",
      "            }\n",
      "\n",
      "        NOTE\n",
      "\n",
      "        The key ``'params'`` is used to store a list of parameter\n",
      "        settings dicts for all the parameter candidates.\n",
      "\n",
      "        The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n",
      "        ``std_score_time`` are all in seconds.\n",
      "\n",
      "        For multi-metric evaluation, the scores for all the scorers are\n",
      "        available in the ``cv_results_`` dict at the keys ending with that\n",
      "        scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n",
      "        above. ('split0_test_precision', 'mean_train_precision' etc.)\n",
      "\n",
      "    best_estimator_ : estimator\n",
      "        Estimator that was chosen by the search, i.e. estimator\n",
      "        which gave highest score (or smallest loss if specified)\n",
      "        on the left out data. Not available if ``refit=False``.\n",
      "\n",
      "        For multi-metric evaluation, this attribute is present only if\n",
      "        ``refit`` is specified.\n",
      "\n",
      "        See ``refit`` parameter for more information on allowed values.\n",
      "\n",
      "    best_score_ : float\n",
      "        Mean cross-validated score of the best_estimator.\n",
      "\n",
      "        For multi-metric evaluation, this is not available if ``refit`` is\n",
      "        ``False``. See ``refit`` parameter for more information.\n",
      "\n",
      "        This attribute is not available if ``refit`` is a function.\n",
      "\n",
      "    best_params_ : dict\n",
      "        Parameter setting that gave the best results on the hold out data.\n",
      "\n",
      "        For multi-metric evaluation, this is not available if ``refit`` is\n",
      "        ``False``. See ``refit`` parameter for more information.\n",
      "\n",
      "    best_index_ : int\n",
      "        The index (of the ``cv_results_`` arrays) which corresponds to the best\n",
      "        candidate parameter setting.\n",
      "\n",
      "        The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n",
      "        the parameter setting for the best model, that gives the highest\n",
      "        mean score (``search.best_score_``).\n",
      "\n",
      "        For multi-metric evaluation, this is not available if ``refit`` is\n",
      "        ``False``. See ``refit`` parameter for more information.\n",
      "\n",
      "    scorer_ : function or a dict\n",
      "        Scorer function used on the held out data to choose the best\n",
      "        parameters for the model.\n",
      "\n",
      "        For multi-metric evaluation, this attribute holds the validated\n",
      "        ``scoring`` dict which maps the scorer key to the scorer callable.\n",
      "\n",
      "    n_splits_ : int\n",
      "        The number of cross-validation splits (folds/iterations).\n",
      "\n",
      "    refit_time_ : float\n",
      "        Seconds used for refitting the best model on the whole dataset.\n",
      "\n",
      "        This is present only if ``refit`` is not False.\n",
      "\n",
      "        .. versionadded:: 0.20\n",
      "\n",
      "    multimetric_ : bool\n",
      "        Whether or not the scorers compute several metrics.\n",
      "\n",
      "    classes_ : ndarray of shape (n_classes,)\n",
      "        The classes labels. This is present only if ``refit`` is specified and\n",
      "        the underlying estimator is a classifier.\n",
      "\n",
      "    n_features_in_ : int\n",
      "        Number of features seen during :term:`fit`. Only defined if\n",
      "        `best_estimator_` is defined (see the documentation for the `refit`\n",
      "        parameter for more details) and that `best_estimator_` exposes\n",
      "        `n_features_in_` when fit.\n",
      "\n",
      "        .. versionadded:: 0.24\n",
      "\n",
      "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "        Names of features seen during :term:`fit`. Only defined if\n",
      "        `best_estimator_` is defined (see the documentation for the `refit`\n",
      "        parameter for more details) and that `best_estimator_` exposes\n",
      "        `feature_names_in_` when fit.\n",
      "\n",
      "        .. versionadded:: 1.0\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    GridSearchCV : Does exhaustive search over a grid of parameters.\n",
      "    ParameterSampler : A generator over parameter settings, constructed from\n",
      "        param_distributions.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The parameters selected are those that maximize the score of the held-out\n",
      "    data, according to the scoring parameter.\n",
      "\n",
      "    If `n_jobs` was set to a value higher than one, the data is copied for each\n",
      "    parameter setting(and not `n_jobs` times). This is done for efficiency\n",
      "    reasons if individual jobs take very little time, but may raise errors if\n",
      "    the dataset is large and not enough memory is available.  A workaround in\n",
      "    this case is to set `pre_dispatch`. Then, the memory is copied only\n",
      "    `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n",
      "    n_jobs`.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.datasets import load_iris\n",
      "    >>> from sklearn.linear_model import LogisticRegression\n",
      "    >>> from sklearn.model_selection import RandomizedSearchCV\n",
      "    >>> from scipy.stats import uniform\n",
      "    >>> iris = load_iris()\n",
      "    >>> logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,\n",
      "    ...                               random_state=0)\n",
      "    >>> distributions = dict(C=uniform(loc=0, scale=4),\n",
      "    ...                      penalty=['l2', 'l1'])\n",
      "    >>> clf = RandomizedSearchCV(logistic, distributions, random_state=0)\n",
      "    >>> search = clf.fit(iris.data, iris.target)\n",
      "    >>> search.best_params_\n",
      "    {'C': 2..., 'penalty': 'l1'}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(rc.__doc__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721fbca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cfba00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
